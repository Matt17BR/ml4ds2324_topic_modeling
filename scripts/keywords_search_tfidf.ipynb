{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching Pubmed's database for papers by using the most common topics found by topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/matteo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/matteo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/matteo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/matteo/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/matteo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'team': 2.631089159966082, 'data': 2.3434070875143007, 'performance': 2.0249533563957662, 'classification': 3.575550768806933, 'ball': 2.6882475738060303, 'match': 2.7181005369557116, 'indicators': 4.422848629194137, 'football': 3.2188758248682006, 'learning': 2.4304184645039304, 'based': 1.783791299578878, 'regression': 4.8283137373023015, 'analysis': 2.2256240518579173, 'used': 0.7298111649315369, 'using': 1.7037485919053417, 'accuracy': 3.170085660698769, 'machine': 2.631089159966082, 'athletes': 4.605170185988092, 'decision': 2.0714733720306593, 'field': 1.4354846053106625, 'model': 2.8134107167600364, 'study': 1.487220279709851, 'players': 3.575550768806933, 'different': 1.0216512475319812, 'outcome': 3.270169119255751, 'injury': 3.324236340526027, 'may': 0.4338645826298623, 'time': 0.11204950380862293, 'features': 2.207274913189721, 'method': 1.8078888511579387, 'models': 2.995732273553991}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Using the nltk package for topic modeling\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import math\n",
    "\n",
    "#Loading the CSV file with references\n",
    "ref_abs = pd.read_csv(os.path.join('..','results','refs_abstracts_sys.csv'))\n",
    "ref_abs.head()\n",
    "\n",
    "#Function for cleaning and preprocessing the abstracts still using NLTK\n",
    "def preprocess_text(texts):\n",
    "    prepositions = set([\n",
    "    \n",
    "    ])\n",
    "    #Filtering out common words that are \"meaningless\" (such as prepositions) using stop words\n",
    "    stop_words = set(stopwords.words('english')).union(prepositions)\n",
    "\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            #Replace hyphens and slashes with spaces, then split the text into words\n",
    "            #Lowercasing, punctuation removal, word tokenization, and stop word filtering\n",
    "            words = text.lower().translate(str.maketrans('-/', '  ')).split()\n",
    "            #Split on other punctuations and filter out stop words\n",
    "            words = [word for part in words for word in part.translate(str.maketrans('', '', string.punctuation)).split() if word not in stop_words]\n",
    "            cleaned_texts.extend(words)\n",
    "\n",
    "    return cleaned_texts\n",
    "#Preprocessing abstracts and counting\n",
    "cleaned_abstracts = preprocess_text(ref_abs['Abstract'])\n",
    "word_counts = Counter(cleaned_abstracts)\n",
    "most_common_words_basic_A = word_counts.most_common(30)\n",
    "document_word_frequencies = most_common_words_basic_A\n",
    "\n",
    "# Load the corpus and calculate the total number of documents\n",
    "documents = brown.fileids()\n",
    "total_documents = len(documents)\n",
    "\n",
    "# Calculate document frequency for each word in your list\n",
    "document_frequencies = {}\n",
    "for word, _ in document_word_frequencies:\n",
    "    document_frequencies[word] = sum(1 for doc in documents if word in brown.words(doc))\n",
    "\n",
    "# Calculate IDF for each word\n",
    "idf_values = {word: math.log(total_documents / (df + 1)) for word, df in document_frequencies.items()}\n",
    "\n",
    "# Example: Calculate TF-IDF for each word in your document\n",
    "tf_idf = {word: freq * idf_values[word] for word, freq in document_word_frequencies}\n",
    "\n",
    "print(idf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'team': 73.6704964790503, 'data': 63.27199136288612, 'performance': 52.64878726628992, 'classification': 67.93546460733172, 'ball': 48.388456328508546, 'match': 43.489608591291386, 'indicators': 66.34272943791206, 'football': 48.28313737302301, 'learning': 34.02585850305503, 'based': 23.189286894525416, 'regression': 62.76807858492992, 'analysis': 26.70748862229501, 'used': 8.757733979178443, 'using': 20.4449831028641, 'accuracy': 38.041027928385226, 'machine': 28.9419807596269, 'athletes': 50.65687204586901, 'decision': 22.786207092337253, 'field': 15.790330658417288, 'model': 30.9475178843604, 'study': 16.359423076808362, 'players': 39.331058456876264, 'different': 11.238163722851793, 'outcome': 35.97186031181326, 'injury': 33.24236340526027, 'may': 4.338645826298623, 'time': 1.1204950380862293, 'features': 22.07274913189721, 'method': 18.078888511579386, 'models': 26.961590461985917}\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
