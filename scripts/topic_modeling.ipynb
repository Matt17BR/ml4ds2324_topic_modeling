{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script will explore topic modelling on the title, abstract and then abstract and title together using 3 different approaches (basic, stemming, and lemmatization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Title Only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russell S, Norvig P</td>\n",
       "      <td>Artificial Intelligence: a modern approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Witten IH, Frank E, Hall MA, et al</td>\n",
       "      <td>Data Mining: practical Machine Learning tools ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zaki MJ, Meira Jr, W</td>\n",
       "      <td>Data Mining and analysis: fundamental concepts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Passfield L, Hopker JG</td>\n",
       "      <td>A mine of information: can sports analytics pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rein R, Memmert D</td>\n",
       "      <td>Big data and tactical analysis in elite soccer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Dalton-Barron NE, McLaren SJ, Black CJ, et al</td>\n",
       "      <td>Identifying contextual influences on training ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>McLaren SJ, Weston M, Smith A, et al</td>\n",
       "      <td>Variability of physical performance and player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Oliveira WK, Jesus K, Andrade AD, et al</td>\n",
       "      <td>Monitoring training load in beach volleyball p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Düking P, Achtzehn S, Holmberg HC, Sperlich B</td>\n",
       "      <td>Integrated framework of load monitoring by a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Austen K</td>\n",
       "      <td>What could derail the wearables revolution? Na...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Author  \\\n",
       "0                              Russell S, Norvig P   \n",
       "1               Witten IH, Frank E, Hall MA, et al   \n",
       "2                             Zaki MJ, Meira Jr, W   \n",
       "3                           Passfield L, Hopker JG   \n",
       "4                                Rein R, Memmert D   \n",
       "..                                             ...   \n",
       "98   Dalton-Barron NE, McLaren SJ, Black CJ, et al   \n",
       "99            McLaren SJ, Weston M, Smith A, et al   \n",
       "100        Oliveira WK, Jesus K, Andrade AD, et al   \n",
       "101  Düking P, Achtzehn S, Holmberg HC, Sperlich B   \n",
       "102                                       Austen K   \n",
       "\n",
       "                                                 Title  \n",
       "0           Artificial Intelligence: a modern approach  \n",
       "1    Data Mining: practical Machine Learning tools ...  \n",
       "2    Data Mining and analysis: fundamental concepts...  \n",
       "3    A mine of information: can sports analytics pr...  \n",
       "4    Big data and tactical analysis in elite soccer...  \n",
       "..                                                 ...  \n",
       "98   Identifying contextual influences on training ...  \n",
       "99   Variability of physical performance and player...  \n",
       "100  Monitoring training load in beach volleyball p...  \n",
       "101  Integrated framework of load monitoring by a c...  \n",
       "102  What could derail the wearables revolution? Na...  \n",
       "\n",
       "[103 rows x 2 columns]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the CSV file with references\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "references = pd.read_csv(os.path.join('..','results','paper_refs.csv'))\n",
    "references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Finding the Most Common Words in Title (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sonayavrumyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the nltk package for topic modeling\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('football', 17), ('neural', 17), ('data', 16), ('training', 16), ('based', 16), ('team', 16), ('performance', 15), ('basketball', 14), ('artificial', 13), ('sports', 13)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "#Function for cleaning and preprocessing the titles\n",
    "def preprocess_titles(titles):\n",
    "    prepositions = set([\n",
    "        \n",
    "    ])\n",
    "    #Filtering out common words that are \"meaningless\" (such as prepositions) using stop words\n",
    "    stop_words = set(stopwords.words('english')).union(prepositions)\n",
    "    \n",
    "    #Lowercasing, punctuation removal, word tokenization, and stop word filtering\n",
    "    cleaned_titles = []\n",
    "    for title in titles:\n",
    "        words = title.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        cleaned_titles.extend(words)\n",
    "\n",
    "    return cleaned_titles\n",
    "\n",
    "#Preprocessing titles and counting\n",
    "cleaned_titles = preprocess_titles(references['Title'])\n",
    "word_counts = Counter(cleaned_titles)\n",
    "most_common_words_basic_T = word_counts.most_common(10)\n",
    "\n",
    "print(most_common_words_basic_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Finding the Most Common Words in Title (Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('footbal', 18), ('network', 18), ('team', 18), ('perform', 17), ('neural', 17), ('data', 16), ('train', 16), ('base', 16), ('sport', 15), ('injuri', 15)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "#Function for cleaning and preprocessing the titles with stemming\n",
    "def preprocess_titles(titles):\n",
    "    stemmer = PorterStemmer()\n",
    "    prepositions = set([\n",
    "        # ... (list all prepositions here) ...\n",
    "    ])\n",
    "    stop_words = set(stopwords.words('english')).union(prepositions)\n",
    "#Lowercasing, punctuation removal, word tokenization, stemming and stop word filtering\n",
    "    cleaned_titles = []\n",
    "    for title in titles:\n",
    "        words = title.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        stemmed_words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "        cleaned_titles.extend(stemmed_words)\n",
    "\n",
    "    return cleaned_titles\n",
    "\n",
    "#Preprocessing titles and counting\n",
    "cleaned_titles = preprocess_titles(references['Title'])\n",
    "word_counts = Counter(cleaned_titles)\n",
    "most_common_words_stemming_T = word_counts.most_common(10)\n",
    "\n",
    "print(most_common_words_stemming_T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Finding the Most Common Words in Title (Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('network', 18), ('team', 18), ('football', 17), ('neural', 17), ('data', 16), ('base', 16), ('sport', 15), ('performance', 15), ('injury', 15), ('basketball', 14)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sonayavrumyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sonayavrumyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraties\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "#Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Function to map NLTK's part of speech tags to those used by WordNet\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "#Function for cleaning and preprocessing the titles with lemmatization\n",
    "def preprocess_titles(titles):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    prepositions = set([\n",
    "        \n",
    "    ])\n",
    "    stop_words = set(stopwords.words('english')).union(prepositions)\n",
    "#Lowercasing, punctuation removal, word tokenization, lemmatizing and stop word filtering\n",
    "    cleaned_titles = []\n",
    "    for title in titles:\n",
    "        words = title.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        pos_tags = pos_tag(words)\n",
    "        lem_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags if word not in stop_words]\n",
    "        cleaned_titles.extend(lem_words)\n",
    "\n",
    "    return cleaned_titles\n",
    "#Preprocessing titles and counting\n",
    "cleaned_titles = preprocess_titles(references['Title'])\n",
    "word_counts = Counter(cleaned_titles)\n",
    "most_common_words_lemm_T = word_counts.most_common(10)\n",
    "\n",
    "print(most_common_words_lemm_T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Displaying All Three Methods Together and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ff4c5 th {\n",
       "  background-color: #f4f4f4;\n",
       "  color: black;\n",
       "}\n",
       "#T_ff4c5_row0_col0, #T_ff4c5_row0_col1, #T_ff4c5_row0_col2, #T_ff4c5_row0_col3, #T_ff4c5_row0_col4, #T_ff4c5_row0_col5, #T_ff4c5_row0_col6, #T_ff4c5_row0_col7, #T_ff4c5_row0_col8, #T_ff4c5_row0_col9, #T_ff4c5_row1_col0, #T_ff4c5_row1_col1, #T_ff4c5_row1_col2, #T_ff4c5_row1_col3, #T_ff4c5_row1_col4, #T_ff4c5_row1_col5, #T_ff4c5_row1_col6, #T_ff4c5_row1_col7, #T_ff4c5_row1_col8, #T_ff4c5_row1_col9, #T_ff4c5_row2_col0, #T_ff4c5_row2_col1, #T_ff4c5_row2_col2, #T_ff4c5_row2_col3, #T_ff4c5_row2_col4, #T_ff4c5_row2_col5, #T_ff4c5_row2_col6, #T_ff4c5_row2_col7, #T_ff4c5_row2_col8, #T_ff4c5_row2_col9 {\n",
       "  background-color: white;\n",
       "  color: black;\n",
       "  border-color: black;\n",
       "  border-style: solid;\n",
       "  border-width: 1px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ff4c5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ff4c5_level0_col0\" class=\"col_heading level0 col0\" >1st Most Common</th>\n",
       "      <th id=\"T_ff4c5_level0_col1\" class=\"col_heading level0 col1\" >2nd Most Common</th>\n",
       "      <th id=\"T_ff4c5_level0_col2\" class=\"col_heading level0 col2\" >3rd Most Common</th>\n",
       "      <th id=\"T_ff4c5_level0_col3\" class=\"col_heading level0 col3\" >4th Most Common</th>\n",
       "      <th id=\"T_ff4c5_level0_col4\" class=\"col_heading level0 col4\" >5th Most Common</th>\n",
       "      <th id=\"T_ff4c5_level0_col5\" class=\"col_heading level0 col5\" >6th Most Common</th>\n",
       "      <th id=\"T_ff4c5_level0_col6\" class=\"col_heading level0 col6\" >7th Most Common</th>\n",
       "      <th id=\"T_ff4c5_level0_col7\" class=\"col_heading level0 col7\" >8th Most Common</th>\n",
       "      <th id=\"T_ff4c5_level0_col8\" class=\"col_heading level0 col8\" >9th Most Common</th>\n",
       "      <th id=\"T_ff4c5_level0_col9\" class=\"col_heading level0 col9\" >10th Most Common</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Method</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ff4c5_level0_row0\" class=\"row_heading level0 row0\" >Basic</th>\n",
       "      <td id=\"T_ff4c5_row0_col0\" class=\"data row0 col0\" >football</td>\n",
       "      <td id=\"T_ff4c5_row0_col1\" class=\"data row0 col1\" >neural</td>\n",
       "      <td id=\"T_ff4c5_row0_col2\" class=\"data row0 col2\" >data</td>\n",
       "      <td id=\"T_ff4c5_row0_col3\" class=\"data row0 col3\" >training</td>\n",
       "      <td id=\"T_ff4c5_row0_col4\" class=\"data row0 col4\" >based</td>\n",
       "      <td id=\"T_ff4c5_row0_col5\" class=\"data row0 col5\" >team</td>\n",
       "      <td id=\"T_ff4c5_row0_col6\" class=\"data row0 col6\" >performance</td>\n",
       "      <td id=\"T_ff4c5_row0_col7\" class=\"data row0 col7\" >basketball</td>\n",
       "      <td id=\"T_ff4c5_row0_col8\" class=\"data row0 col8\" >artificial</td>\n",
       "      <td id=\"T_ff4c5_row0_col9\" class=\"data row0 col9\" >sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ff4c5_level0_row1\" class=\"row_heading level0 row1\" >Stemming</th>\n",
       "      <td id=\"T_ff4c5_row1_col0\" class=\"data row1 col0\" >footbal</td>\n",
       "      <td id=\"T_ff4c5_row1_col1\" class=\"data row1 col1\" >network</td>\n",
       "      <td id=\"T_ff4c5_row1_col2\" class=\"data row1 col2\" >team</td>\n",
       "      <td id=\"T_ff4c5_row1_col3\" class=\"data row1 col3\" >perform</td>\n",
       "      <td id=\"T_ff4c5_row1_col4\" class=\"data row1 col4\" >neural</td>\n",
       "      <td id=\"T_ff4c5_row1_col5\" class=\"data row1 col5\" >data</td>\n",
       "      <td id=\"T_ff4c5_row1_col6\" class=\"data row1 col6\" >train</td>\n",
       "      <td id=\"T_ff4c5_row1_col7\" class=\"data row1 col7\" >base</td>\n",
       "      <td id=\"T_ff4c5_row1_col8\" class=\"data row1 col8\" >sport</td>\n",
       "      <td id=\"T_ff4c5_row1_col9\" class=\"data row1 col9\" >injuri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ff4c5_level0_row2\" class=\"row_heading level0 row2\" >Lemmatization</th>\n",
       "      <td id=\"T_ff4c5_row2_col0\" class=\"data row2 col0\" >network</td>\n",
       "      <td id=\"T_ff4c5_row2_col1\" class=\"data row2 col1\" >team</td>\n",
       "      <td id=\"T_ff4c5_row2_col2\" class=\"data row2 col2\" >football</td>\n",
       "      <td id=\"T_ff4c5_row2_col3\" class=\"data row2 col3\" >neural</td>\n",
       "      <td id=\"T_ff4c5_row2_col4\" class=\"data row2 col4\" >data</td>\n",
       "      <td id=\"T_ff4c5_row2_col5\" class=\"data row2 col5\" >base</td>\n",
       "      <td id=\"T_ff4c5_row2_col6\" class=\"data row2 col6\" >sport</td>\n",
       "      <td id=\"T_ff4c5_row2_col7\" class=\"data row2 col7\" >performance</td>\n",
       "      <td id=\"T_ff4c5_row2_col8\" class=\"data row2 col8\" >injury</td>\n",
       "      <td id=\"T_ff4c5_row2_col9\" class=\"data row2 col9\" >basketball</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14dfd2090>"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using pandas to display the 10 most common words in a table for ease of comparison\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame({\n",
    "    'Method': ['Basic', 'Stemming', 'Lemmatization'],\n",
    "    '1st Most Common': [most_common_words_basic_T[0][0], most_common_words_stemming_T[0][0], most_common_words_lemm_T[0][0]],\n",
    "    '2nd Most Common': [most_common_words_basic_T[1][0], most_common_words_stemming_T[1][0], most_common_words_lemm_T[1][0]],\n",
    "    '3rd Most Common': [most_common_words_basic_T[2][0], most_common_words_stemming_T[2][0], most_common_words_lemm_T[2][0]],\n",
    "    '4th Most Common': [most_common_words_basic_T[3][0], most_common_words_stemming_T[3][0], most_common_words_lemm_T[3][0]],\n",
    "    '5th Most Common': [most_common_words_basic_T[4][0], most_common_words_stemming_T[4][0], most_common_words_lemm_T[4][0]],\n",
    "    '6th Most Common': [most_common_words_basic_T[5][0], most_common_words_stemming_T[5][0], most_common_words_lemm_T[5][0]],\n",
    "    '7th Most Common': [most_common_words_basic_T[6][0], most_common_words_stemming_T[6][0], most_common_words_lemm_T[6][0]],\n",
    "    '8th Most Common': [most_common_words_basic_T[7][0], most_common_words_stemming_T[7][0], most_common_words_lemm_T[7][0]],\n",
    "    '9th Most Common': [most_common_words_basic_T[8][0], most_common_words_stemming_T[8][0], most_common_words_lemm_T[8][0]],\n",
    "    '10th Most Common': [most_common_words_basic_T[9][0], most_common_words_stemming_T[9][0], most_common_words_lemm_T[9][0]]\n",
    "})\n",
    "results_df.set_index('Method', inplace=True)\n",
    "\n",
    "#Applying basic styling to the table \n",
    "styled_df = results_df.style.set_properties(**{\n",
    "    'background-color': 'white',  #Background color\n",
    "    'color': 'black',             #Font color\n",
    "    'border-color': 'black',      #Border color\n",
    "    'border-style': 'solid',      #Border style\n",
    "    'border-width': '1px'         #Border width\n",
    "}).set_table_styles([{\n",
    "    'selector': 'th',\n",
    "    'props': [('background-color', '#f4f4f4'), ('color', 'black')]  #Header styling\n",
    "}])\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conclusion***: Lemmatization considers the context and part of speech of a word, leading to more accurate results as it reduces words to their dictionary form, but requires more computational power. Stemming is robust for search and indexing purposes as the exact form of a word is less important and is faster. It's interesting to note that some words such as \"team\" and \"basketball\" have drastic differences across the methods, whereas words such as \"football\" have more consistent positioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Abstract Only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russell S, Norvig P</td>\n",
       "      <td>Artificial Intelligence: a modern approach</td>\n",
       "      <td>From the enraged robots in the 1920 play R.U.R...</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Witten IH, Frank E, Hall MA, et al</td>\n",
       "      <td>Data Mining: practical Machine Learning tools ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zaki MJ, Meira Jr, W</td>\n",
       "      <td>Data Mining and analysis: fundamental concepts...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Passfield L, Hopker JG</td>\n",
       "      <td>A mine of information: can sports analytics pr...</td>\n",
       "      <td>This paper explores the notion that the availa...</td>\n",
       "      <td>2017.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rein R, Memmert D</td>\n",
       "      <td>Big data and tactical analysis in elite soccer...</td>\n",
       "      <td>Until recently tactical analysis in elite socc...</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Dalton-Barron NE, McLaren SJ, Black CJ, et al</td>\n",
       "      <td>Identifying contextual influences on training ...</td>\n",
       "      <td>Dalton-Barron, NE, McLaren, SJ, Black, CJ, Gra...</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>McLaren SJ, Weston M, Smith A, et al</td>\n",
       "      <td>Variability of physical performance and player...</td>\n",
       "      <td>The aims of this study were to establish sourc...</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Oliveira WK, Jesus K, Andrade AD, et al</td>\n",
       "      <td>Monitoring training load in beach volleyball p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Düking P, Achtzehn S, Holmberg HC, Sperlich B</td>\n",
       "      <td>Integrated framework of load monitoring by a c...</td>\n",
       "      <td>Athletes schedule their training and recovery ...</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Austen K</td>\n",
       "      <td>What could derail the wearables revolution? Na...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Author  \\\n",
       "0                              Russell S, Norvig P   \n",
       "1               Witten IH, Frank E, Hall MA, et al   \n",
       "2                             Zaki MJ, Meira Jr, W   \n",
       "3                           Passfield L, Hopker JG   \n",
       "4                                Rein R, Memmert D   \n",
       "..                                             ...   \n",
       "98   Dalton-Barron NE, McLaren SJ, Black CJ, et al   \n",
       "99            McLaren SJ, Weston M, Smith A, et al   \n",
       "100        Oliveira WK, Jesus K, Andrade AD, et al   \n",
       "101  Düking P, Achtzehn S, Holmberg HC, Sperlich B   \n",
       "102                                       Austen K   \n",
       "\n",
       "                                                 Title  \\\n",
       "0           Artificial Intelligence: a modern approach   \n",
       "1    Data Mining: practical Machine Learning tools ...   \n",
       "2    Data Mining and analysis: fundamental concepts...   \n",
       "3    A mine of information: can sports analytics pr...   \n",
       "4    Big data and tactical analysis in elite soccer...   \n",
       "..                                                 ...   \n",
       "98   Identifying contextual influences on training ...   \n",
       "99   Variability of physical performance and player...   \n",
       "100  Monitoring training load in beach volleyball p...   \n",
       "101  Integrated framework of load monitoring by a c...   \n",
       "102  What could derail the wearables revolution? Na...   \n",
       "\n",
       "                                              Abstract    Year  \n",
       "0    From the enraged robots in the 1920 play R.U.R...  2015.0  \n",
       "1                                                  NaN     NaN  \n",
       "2                                                  NaN     NaN  \n",
       "3    This paper explores the notion that the availa...  2017.0  \n",
       "4    Until recently tactical analysis in elite socc...  2016.0  \n",
       "..                                                 ...     ...  \n",
       "98   Dalton-Barron, NE, McLaren, SJ, Black, CJ, Gra...  2021.0  \n",
       "99   The aims of this study were to establish sourc...  2021.0  \n",
       "100                                                NaN     NaN  \n",
       "101  Athletes schedule their training and recovery ...  2018.0  \n",
       "102                                                NaN  2015.0  \n",
       "\n",
       "[103 rows x 4 columns]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the CSV file with references\n",
    "ref_abs = pd.read_csv(os.path.join('..','results','paper_refs_abstracts.csv'))\n",
    "ref_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 2.1 Finding the Most Common Words in Abstract (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 66), ('training', 51), ('team', 50), ('performance', 50), ('match', 31), ('risk', 26), ('injury', 26), ('players', 26), ('used', 25), ('sports', 24)]\n"
     ]
    }
   ],
   "source": [
    "#Function for cleaning and preprocessing the abstracts still using NLTK\n",
    "def preprocess_text(texts):\n",
    "    prepositions = set([\n",
    "    \n",
    "    ])\n",
    "    #Filtering out common words that are \"meaningless\" (such as prepositions) using stop words\n",
    "    stop_words = set(stopwords.words('english')).union(prepositions)\n",
    "\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            #Replace hyphens and slashes with spaces, then split the text into words\n",
    "            #Lowercasing, punctuation removal, word tokenization, and stop word filtering\n",
    "            words = text.lower().translate(str.maketrans('-/', '  ')).split()\n",
    "            #Split on other punctuations and filter out stop words\n",
    "            words = [word for part in words for word in part.translate(str.maketrans('', '', string.punctuation)).split() if word not in stop_words]\n",
    "            cleaned_texts.extend(words)\n",
    "\n",
    "    return cleaned_texts\n",
    "#Preprocessing abstracts and counting\n",
    "cleaned_abstracts = preprocess_text(ref_abs['Abstract'])\n",
    "word_counts = Counter(cleaned_abstracts)\n",
    "most_common_words_basic_A = word_counts.most_common(10)\n",
    "\n",
    "print(most_common_words_basic_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Method is limited, as words like \"sport\" and \"sports\" will be counted separately. We will further apply lemmatization and stemming to address this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Finding the Most Common Words in Abstract (Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 66), ('perform', 62), ('train', 60), ('use', 59), ('team', 58), ('sport', 42), ('player', 41), ('injuri', 36), ('studi', 35), ('model', 35)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "#Function for cleaning and preprocessing the abstracts\n",
    "def preprocess_text(texts):\n",
    "    prepositions = set([\n",
    "        \n",
    "    ])\n",
    "    #Filtering out common words that are \"meaningless\" (such as prepositions) using stop words\n",
    "    stop_words = set(stopwords.words('english')).union(prepositions)\n",
    "    stemmer = PorterStemmer()\n",
    "    #Using PorterStemmer() for stemming and further lowercasing, punctuation removal, word tokenization, and stop word filtering\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            #Replace hyphens and slashes with spaces\n",
    "            words = text.lower().translate(str.maketrans('-/', '  ')).split()\n",
    "            words = [word for part in words for word in part.translate(str.maketrans('', '', string.punctuation)).split() if word not in stop_words]\n",
    "            stemmed_words = [stemmer.stem(word) for word in words]\n",
    "            cleaned_texts.extend(stemmed_words)\n",
    "\n",
    "    return cleaned_texts\n",
    "#Preprocessing abstracts and counting\n",
    "cleaned_abstracts = preprocess_text(ref_abs['Abstract'])\n",
    "word_counts = Counter(cleaned_abstracts)\n",
    "most_common_words_stemming_A = word_counts.most_common(10)\n",
    "\n",
    "print(most_common_words_stemming_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Most Common words in Abstract (Lematization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sonayavrumyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sonayavrumyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 66), ('team', 58), ('use', 51), ('training', 51), ('performance', 51), ('sport', 42), ('player', 41), ('injury', 36), ('study', 35), ('model', 35)]\n"
     ]
    }
   ],
   "source": [
    "#Ensure necessary NLTK resources are downloaded\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#Function to map NLTK's part of speech tags to those used by WordNet\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to the first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "#Function for cleaning and preprocessing the abstracts with lemmatization\n",
    "def preprocess_text(texts):\n",
    "    prepositions = set([\n",
    "       \n",
    "    ])\n",
    "    stop_words = set(stopwords.words('english')).union(prepositions)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "#Lowercasing, punctuation removal, word tokenization, lemmatizing and stop word filtering\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            #Replace hyphens and slashes with spaces\n",
    "            words = text.lower().translate(str.maketrans('-/', '  ')).split()\n",
    "            words = [word for part in words for word in part.translate(str.maketrans('', '', string.punctuation)).split() if word not in stop_words]\n",
    "            lem_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "            cleaned_texts.extend(lem_words)\n",
    "\n",
    "    return cleaned_texts\n",
    "#Preprocessing the abstracts and counting\n",
    "cleaned_abstracts = preprocess_text(ref_abs['Abstract'])\n",
    "word_counts = Counter(cleaned_abstracts)\n",
    "most_common_words_lemm_A = word_counts.most_common(10)\n",
    "\n",
    "print(most_common_words_lemm_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Displaying All Three Methods Together and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_255fd th {\n",
       "  background-color: #f4f4f4;\n",
       "  color: black;\n",
       "}\n",
       "#T_255fd_row0_col0, #T_255fd_row0_col1, #T_255fd_row0_col2, #T_255fd_row0_col3, #T_255fd_row0_col4, #T_255fd_row0_col5, #T_255fd_row0_col6, #T_255fd_row0_col7, #T_255fd_row0_col8, #T_255fd_row0_col9, #T_255fd_row1_col0, #T_255fd_row1_col1, #T_255fd_row1_col2, #T_255fd_row1_col3, #T_255fd_row1_col4, #T_255fd_row1_col5, #T_255fd_row1_col6, #T_255fd_row1_col7, #T_255fd_row1_col8, #T_255fd_row1_col9, #T_255fd_row2_col0, #T_255fd_row2_col1, #T_255fd_row2_col2, #T_255fd_row2_col3, #T_255fd_row2_col4, #T_255fd_row2_col5, #T_255fd_row2_col6, #T_255fd_row2_col7, #T_255fd_row2_col8, #T_255fd_row2_col9 {\n",
       "  background-color: white;\n",
       "  color: black;\n",
       "  border-color: black;\n",
       "  border-style: solid;\n",
       "  border-width: 1px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_255fd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_255fd_level0_col0\" class=\"col_heading level0 col0\" >1st Most Common</th>\n",
       "      <th id=\"T_255fd_level0_col1\" class=\"col_heading level0 col1\" >2nd Most Common</th>\n",
       "      <th id=\"T_255fd_level0_col2\" class=\"col_heading level0 col2\" >3rd Most Common</th>\n",
       "      <th id=\"T_255fd_level0_col3\" class=\"col_heading level0 col3\" >4th Most Common</th>\n",
       "      <th id=\"T_255fd_level0_col4\" class=\"col_heading level0 col4\" >5th Most Common</th>\n",
       "      <th id=\"T_255fd_level0_col5\" class=\"col_heading level0 col5\" >6th Most Common</th>\n",
       "      <th id=\"T_255fd_level0_col6\" class=\"col_heading level0 col6\" >7th Most Common</th>\n",
       "      <th id=\"T_255fd_level0_col7\" class=\"col_heading level0 col7\" >8th Most Common</th>\n",
       "      <th id=\"T_255fd_level0_col8\" class=\"col_heading level0 col8\" >9th Most Common</th>\n",
       "      <th id=\"T_255fd_level0_col9\" class=\"col_heading level0 col9\" >10th Most Common</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Method</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_255fd_level0_row0\" class=\"row_heading level0 row0\" >Basic</th>\n",
       "      <td id=\"T_255fd_row0_col0\" class=\"data row0 col0\" >data</td>\n",
       "      <td id=\"T_255fd_row0_col1\" class=\"data row0 col1\" >training</td>\n",
       "      <td id=\"T_255fd_row0_col2\" class=\"data row0 col2\" >team</td>\n",
       "      <td id=\"T_255fd_row0_col3\" class=\"data row0 col3\" >performance</td>\n",
       "      <td id=\"T_255fd_row0_col4\" class=\"data row0 col4\" >match</td>\n",
       "      <td id=\"T_255fd_row0_col5\" class=\"data row0 col5\" >risk</td>\n",
       "      <td id=\"T_255fd_row0_col6\" class=\"data row0 col6\" >injury</td>\n",
       "      <td id=\"T_255fd_row0_col7\" class=\"data row0 col7\" >players</td>\n",
       "      <td id=\"T_255fd_row0_col8\" class=\"data row0 col8\" >used</td>\n",
       "      <td id=\"T_255fd_row0_col9\" class=\"data row0 col9\" >sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_255fd_level0_row1\" class=\"row_heading level0 row1\" >Stemming</th>\n",
       "      <td id=\"T_255fd_row1_col0\" class=\"data row1 col0\" >data</td>\n",
       "      <td id=\"T_255fd_row1_col1\" class=\"data row1 col1\" >perform</td>\n",
       "      <td id=\"T_255fd_row1_col2\" class=\"data row1 col2\" >train</td>\n",
       "      <td id=\"T_255fd_row1_col3\" class=\"data row1 col3\" >use</td>\n",
       "      <td id=\"T_255fd_row1_col4\" class=\"data row1 col4\" >team</td>\n",
       "      <td id=\"T_255fd_row1_col5\" class=\"data row1 col5\" >sport</td>\n",
       "      <td id=\"T_255fd_row1_col6\" class=\"data row1 col6\" >player</td>\n",
       "      <td id=\"T_255fd_row1_col7\" class=\"data row1 col7\" >injuri</td>\n",
       "      <td id=\"T_255fd_row1_col8\" class=\"data row1 col8\" >studi</td>\n",
       "      <td id=\"T_255fd_row1_col9\" class=\"data row1 col9\" >model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_255fd_level0_row2\" class=\"row_heading level0 row2\" >Lemmatization</th>\n",
       "      <td id=\"T_255fd_row2_col0\" class=\"data row2 col0\" >data</td>\n",
       "      <td id=\"T_255fd_row2_col1\" class=\"data row2 col1\" >team</td>\n",
       "      <td id=\"T_255fd_row2_col2\" class=\"data row2 col2\" >use</td>\n",
       "      <td id=\"T_255fd_row2_col3\" class=\"data row2 col3\" >training</td>\n",
       "      <td id=\"T_255fd_row2_col4\" class=\"data row2 col4\" >performance</td>\n",
       "      <td id=\"T_255fd_row2_col5\" class=\"data row2 col5\" >sport</td>\n",
       "      <td id=\"T_255fd_row2_col6\" class=\"data row2 col6\" >player</td>\n",
       "      <td id=\"T_255fd_row2_col7\" class=\"data row2 col7\" >injury</td>\n",
       "      <td id=\"T_255fd_row2_col8\" class=\"data row2 col8\" >study</td>\n",
       "      <td id=\"T_255fd_row2_col9\" class=\"data row2 col9\" >model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14026eb50>"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using pandas for displaying the results side by side in a table for ease of comparison\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame({\n",
    "    'Method': ['Basic', 'Stemming', 'Lemmatization'],\n",
    "    '1st Most Common': [most_common_words_basic_A[0][0], most_common_words_stemming_A[0][0], most_common_words_lemm_A[0][0]],\n",
    "    '2nd Most Common': [most_common_words_basic_A[1][0], most_common_words_stemming_A[1][0], most_common_words_lemm_A[1][0]],\n",
    "    '3rd Most Common': [most_common_words_basic_A[2][0], most_common_words_stemming_A[2][0], most_common_words_lemm_A[2][0]],\n",
    "    '4th Most Common': [most_common_words_basic_A[3][0], most_common_words_stemming_A[3][0], most_common_words_lemm_A[3][0]],\n",
    "    '5th Most Common': [most_common_words_basic_A[4][0], most_common_words_stemming_A[4][0], most_common_words_lemm_A[4][0]],\n",
    "    '6th Most Common': [most_common_words_basic_A[5][0], most_common_words_stemming_A[5][0], most_common_words_lemm_A[5][0]],\n",
    "    '7th Most Common': [most_common_words_basic_A[6][0], most_common_words_stemming_A[6][0], most_common_words_lemm_A[6][0]],\n",
    "    '8th Most Common': [most_common_words_basic_A[7][0], most_common_words_stemming_A[7][0], most_common_words_lemm_A[7][0]],\n",
    "    '9th Most Common': [most_common_words_basic_A[8][0], most_common_words_stemming_A[8][0], most_common_words_lemm_A[8][0]],\n",
    "    '10th Most Common': [most_common_words_basic_A[9][0], most_common_words_stemming_A[9][0], most_common_words_lemm_A[9][0]]\n",
    "})\n",
    "results_df.set_index('Method', inplace=True)\n",
    "\n",
    "#Applying basic styling\n",
    "styled_df = results_df.style.set_properties(**{\n",
    "    'background-color': 'white',  #Background color\n",
    "    'color': 'black',             #Font color\n",
    "    'border-color': 'black',      #Border color\n",
    "    'border-style': 'solid',      #Border style\n",
    "    'border-width': '1px'         #Border width\n",
    "}).set_table_styles([{\n",
    "    'selector': 'th',\n",
    "    'props': [('background-color', '#f4f4f4'), ('color', 'black')]  #Header styling\n",
    "}])\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conclusion:***  Lemmatization considers the context and part of speech of a word, leading to more accurate results as it reduces words to their dictionary form, but requires more computational power. Stemming is robust for search and indexing purposes as the exact form of a word is less important and is faster. It's interesting to note that the word \"data is consistently the most common word across all 3 methods in the abstract and some words like \"risk\" have greatly varying positioning. Overall, compared to titles the differences are less extreme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accross both title and abstract, the most common words vary significantly. In the 3rd part we will discuss what are the most common words in both together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Title and Abstract Together"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
