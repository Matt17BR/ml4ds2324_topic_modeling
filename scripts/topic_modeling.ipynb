{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script will explore topic modelling on the title, abstract and then abstract and title together using 3 different approaches (basic, stemming, and lemmatization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Title Only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>López-Valenciano A, Ayala F, Puerta JM, et al</td>\n",
       "      <td>A preventive model for muscle injuries: a nove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Li C</td>\n",
       "      <td>Predict the neural network mathematical model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lu G</td>\n",
       "      <td>Evaluation model of young basketball players ’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wu L</td>\n",
       "      <td>The participating team ’s technical analysis o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zhang Q</td>\n",
       "      <td>Prediction based on basketball competition vid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Author  \\\n",
       "0  López-Valenciano A, Ayala F, Puerta JM, et al   \n",
       "1                                           Li C   \n",
       "2                                           Lu G   \n",
       "3                                           Wu L   \n",
       "4                                        Zhang Q   \n",
       "\n",
       "                                               Title  \n",
       "0  A preventive model for muscle injuries: a nove...  \n",
       "1  Predict the neural network mathematical model ...  \n",
       "2  Evaluation model of young basketball players ’...  \n",
       "3  The participating team ’s technical analysis o...  \n",
       "4  Prediction based on basketball competition vid...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the CSV file with references\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "references = pd.read_csv(os.path.join('..','results','refs_systematic.csv'))\n",
    "references.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Finding the Most Common Words in Title (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/matteo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the nltk package for topic modeling\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neural', 17), ('based', 15), ('team', 13), ('basketball', 12), ('network', 11), ('football', 11), ('data', 10), ('model', 9), ('using', 9), ('artificial', 8)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "#Function for cleaning and preprocessing the titles\n",
    "def preprocess_titles(titles):\n",
    "    prepositions = set([\n",
    "        \n",
    "    ])\n",
    "    #Filtering out common words that are \"meaningless\" (such as prepositions) using stop words\n",
    "    stop_words = set(stopwords.words('english')).union(prepositions)\n",
    "    \n",
    "    #Lowercasing, punctuation removal, word tokenization, and stop word filtering\n",
    "    cleaned_titles = []\n",
    "    for title in titles:\n",
    "        words = title.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        cleaned_titles.extend(words)\n",
    "\n",
    "    return cleaned_titles\n",
    "\n",
    "#Preprocessing titles and counting\n",
    "cleaned_titles = preprocess_titles(references['Title'])\n",
    "word_counts = Counter(cleaned_titles)\n",
    "most_common_words_basic_T = word_counts.most_common(10)\n",
    "\n",
    "print(most_common_words_basic_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Finding the Most Common Words in Title (Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('network', 18), ('neural', 17), ('base', 15), ('team', 15), ('basketbal', 12), ('footbal', 12), ('model', 11), ('predict', 11), ('data', 10), ('perform', 9)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "#Function for cleaning and preprocessing the titles with stemming\n",
    "def preprocess_titles(titles):\n",
    "    stemmer = PorterStemmer()\n",
    "    prepositions = set([\n",
    "        # ... (list all prepositions here) ...\n",
    "    ])\n",
    "    stop_words = set(stopwords.words('english')).union(prepositions)\n",
    "#Lowercasing, punctuation removal, word tokenization, stemming and stop word filtering\n",
    "    cleaned_titles = []\n",
    "    for title in titles:\n",
    "        words = title.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        stemmed_words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "        cleaned_titles.extend(stemmed_words)\n",
    "\n",
    "    return cleaned_titles\n",
    "\n",
    "#Preprocessing titles and counting\n",
    "cleaned_titles = preprocess_titles(references['Title'])\n",
    "word_counts = Counter(cleaned_titles)\n",
    "most_common_words_stemming_T = word_counts.most_common(10)\n",
    "\n",
    "print(most_common_words_stemming_T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Finding the Most Common Words in Title (Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('network', 18), ('neural', 17), ('base', 15), ('team', 15), ('basketball', 12), ('football', 11), ('model', 10), ('data', 10), ('use', 9), ('game', 8)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/matteo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/matteo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraties\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "#Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Function to map NLTK's part of speech tags to those used by WordNet\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "#Function for cleaning and preprocessing the titles with lemmatization\n",
    "def preprocess_titles(titles):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    prepositions = set([\n",
    "        \n",
    "    ])\n",
    "    stop_words = set(stopwords.words('english')).union(prepositions)\n",
    "#Lowercasing, punctuation removal, word tokenization, lemmatizing and stop word filtering\n",
    "    cleaned_titles = []\n",
    "    for title in titles:\n",
    "        words = title.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        pos_tags = pos_tag(words)\n",
    "        lem_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags if word not in stop_words]\n",
    "        cleaned_titles.extend(lem_words)\n",
    "\n",
    "    return cleaned_titles\n",
    "#Preprocessing titles and counting\n",
    "cleaned_titles = preprocess_titles(references['Title'])\n",
    "word_counts = Counter(cleaned_titles)\n",
    "most_common_words_lemm_T = word_counts.most_common(10)\n",
    "\n",
    "print(most_common_words_lemm_T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Displaying All Three Methods Together and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_eaec9 th {\n",
       "  background-color: #f4f4f4;\n",
       "  color: black;\n",
       "}\n",
       "#T_eaec9_row0_col0, #T_eaec9_row0_col1, #T_eaec9_row0_col2, #T_eaec9_row0_col3, #T_eaec9_row0_col4, #T_eaec9_row0_col5, #T_eaec9_row0_col6, #T_eaec9_row0_col7, #T_eaec9_row0_col8, #T_eaec9_row0_col9, #T_eaec9_row1_col0, #T_eaec9_row1_col1, #T_eaec9_row1_col2, #T_eaec9_row1_col3, #T_eaec9_row1_col4, #T_eaec9_row1_col5, #T_eaec9_row1_col6, #T_eaec9_row1_col7, #T_eaec9_row1_col8, #T_eaec9_row1_col9, #T_eaec9_row2_col0, #T_eaec9_row2_col1, #T_eaec9_row2_col2, #T_eaec9_row2_col3, #T_eaec9_row2_col4, #T_eaec9_row2_col5, #T_eaec9_row2_col6, #T_eaec9_row2_col7, #T_eaec9_row2_col8, #T_eaec9_row2_col9 {\n",
       "  background-color: white;\n",
       "  color: black;\n",
       "  border-color: black;\n",
       "  border-style: solid;\n",
       "  border-width: 1px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_eaec9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_eaec9_level0_col0\" class=\"col_heading level0 col0\" >1st Most Common</th>\n",
       "      <th id=\"T_eaec9_level0_col1\" class=\"col_heading level0 col1\" >2nd Most Common</th>\n",
       "      <th id=\"T_eaec9_level0_col2\" class=\"col_heading level0 col2\" >3rd Most Common</th>\n",
       "      <th id=\"T_eaec9_level0_col3\" class=\"col_heading level0 col3\" >4th Most Common</th>\n",
       "      <th id=\"T_eaec9_level0_col4\" class=\"col_heading level0 col4\" >5th Most Common</th>\n",
       "      <th id=\"T_eaec9_level0_col5\" class=\"col_heading level0 col5\" >6th Most Common</th>\n",
       "      <th id=\"T_eaec9_level0_col6\" class=\"col_heading level0 col6\" >7th Most Common</th>\n",
       "      <th id=\"T_eaec9_level0_col7\" class=\"col_heading level0 col7\" >8th Most Common</th>\n",
       "      <th id=\"T_eaec9_level0_col8\" class=\"col_heading level0 col8\" >9th Most Common</th>\n",
       "      <th id=\"T_eaec9_level0_col9\" class=\"col_heading level0 col9\" >10th Most Common</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Method</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_eaec9_level0_row0\" class=\"row_heading level0 row0\" >Basic</th>\n",
       "      <td id=\"T_eaec9_row0_col0\" class=\"data row0 col0\" >neural</td>\n",
       "      <td id=\"T_eaec9_row0_col1\" class=\"data row0 col1\" >based</td>\n",
       "      <td id=\"T_eaec9_row0_col2\" class=\"data row0 col2\" >team</td>\n",
       "      <td id=\"T_eaec9_row0_col3\" class=\"data row0 col3\" >basketball</td>\n",
       "      <td id=\"T_eaec9_row0_col4\" class=\"data row0 col4\" >network</td>\n",
       "      <td id=\"T_eaec9_row0_col5\" class=\"data row0 col5\" >football</td>\n",
       "      <td id=\"T_eaec9_row0_col6\" class=\"data row0 col6\" >data</td>\n",
       "      <td id=\"T_eaec9_row0_col7\" class=\"data row0 col7\" >model</td>\n",
       "      <td id=\"T_eaec9_row0_col8\" class=\"data row0 col8\" >using</td>\n",
       "      <td id=\"T_eaec9_row0_col9\" class=\"data row0 col9\" >artificial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eaec9_level0_row1\" class=\"row_heading level0 row1\" >Stemming</th>\n",
       "      <td id=\"T_eaec9_row1_col0\" class=\"data row1 col0\" >network</td>\n",
       "      <td id=\"T_eaec9_row1_col1\" class=\"data row1 col1\" >neural</td>\n",
       "      <td id=\"T_eaec9_row1_col2\" class=\"data row1 col2\" >base</td>\n",
       "      <td id=\"T_eaec9_row1_col3\" class=\"data row1 col3\" >team</td>\n",
       "      <td id=\"T_eaec9_row1_col4\" class=\"data row1 col4\" >basketbal</td>\n",
       "      <td id=\"T_eaec9_row1_col5\" class=\"data row1 col5\" >footbal</td>\n",
       "      <td id=\"T_eaec9_row1_col6\" class=\"data row1 col6\" >model</td>\n",
       "      <td id=\"T_eaec9_row1_col7\" class=\"data row1 col7\" >predict</td>\n",
       "      <td id=\"T_eaec9_row1_col8\" class=\"data row1 col8\" >data</td>\n",
       "      <td id=\"T_eaec9_row1_col9\" class=\"data row1 col9\" >perform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eaec9_level0_row2\" class=\"row_heading level0 row2\" >Lemmatization</th>\n",
       "      <td id=\"T_eaec9_row2_col0\" class=\"data row2 col0\" >network</td>\n",
       "      <td id=\"T_eaec9_row2_col1\" class=\"data row2 col1\" >neural</td>\n",
       "      <td id=\"T_eaec9_row2_col2\" class=\"data row2 col2\" >base</td>\n",
       "      <td id=\"T_eaec9_row2_col3\" class=\"data row2 col3\" >team</td>\n",
       "      <td id=\"T_eaec9_row2_col4\" class=\"data row2 col4\" >basketball</td>\n",
       "      <td id=\"T_eaec9_row2_col5\" class=\"data row2 col5\" >football</td>\n",
       "      <td id=\"T_eaec9_row2_col6\" class=\"data row2 col6\" >model</td>\n",
       "      <td id=\"T_eaec9_row2_col7\" class=\"data row2 col7\" >data</td>\n",
       "      <td id=\"T_eaec9_row2_col8\" class=\"data row2 col8\" >use</td>\n",
       "      <td id=\"T_eaec9_row2_col9\" class=\"data row2 col9\" >game</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f64acad49d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using pandas to display the 10 most common words in a table for ease of comparison\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame({\n",
    "    'Method': ['Basic', 'Stemming', 'Lemmatization'],\n",
    "    '1st Most Common': [most_common_words_basic_T[0][0], most_common_words_stemming_T[0][0], most_common_words_lemm_T[0][0]],\n",
    "    '2nd Most Common': [most_common_words_basic_T[1][0], most_common_words_stemming_T[1][0], most_common_words_lemm_T[1][0]],\n",
    "    '3rd Most Common': [most_common_words_basic_T[2][0], most_common_words_stemming_T[2][0], most_common_words_lemm_T[2][0]],\n",
    "    '4th Most Common': [most_common_words_basic_T[3][0], most_common_words_stemming_T[3][0], most_common_words_lemm_T[3][0]],\n",
    "    '5th Most Common': [most_common_words_basic_T[4][0], most_common_words_stemming_T[4][0], most_common_words_lemm_T[4][0]],\n",
    "    '6th Most Common': [most_common_words_basic_T[5][0], most_common_words_stemming_T[5][0], most_common_words_lemm_T[5][0]],\n",
    "    '7th Most Common': [most_common_words_basic_T[6][0], most_common_words_stemming_T[6][0], most_common_words_lemm_T[6][0]],\n",
    "    '8th Most Common': [most_common_words_basic_T[7][0], most_common_words_stemming_T[7][0], most_common_words_lemm_T[7][0]],\n",
    "    '9th Most Common': [most_common_words_basic_T[8][0], most_common_words_stemming_T[8][0], most_common_words_lemm_T[8][0]],\n",
    "    '10th Most Common': [most_common_words_basic_T[9][0], most_common_words_stemming_T[9][0], most_common_words_lemm_T[9][0]]\n",
    "})\n",
    "results_df.set_index('Method', inplace=True)\n",
    "\n",
    "#Applying basic styling to the table \n",
    "styled_df = results_df.style.set_properties(**{\n",
    "    'background-color': 'white',  #Background color\n",
    "    'color': 'black',             #Font color\n",
    "    'border-color': 'black',      #Border color\n",
    "    'border-style': 'solid',      #Border style\n",
    "    'border-width': '1px'         #Border width\n",
    "}).set_table_styles([{\n",
    "    'selector': 'th',\n",
    "    'props': [('background-color', '#f4f4f4'), ('color', 'black')]  #Header styling\n",
    "}])\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conclusion***: Lemmatization considers the context and part of speech of a word, leading to more accurate results as it reduces words to their dictionary form, but requires more computational power. Stemming is robust for search and indexing purposes as the exact form of a word is less important and is faster. It's interesting to note that some words such as \"team\" and \"basketball\" have drastic differences across the methods, whereas words such as \"football\" have more consistent positioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Abstract Only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>López-Valenciano A, Ayala F, Puerta JM, et al</td>\n",
       "      <td>A preventive model for muscle injuries: a nove...</td>\n",
       "      <td>The application of contemporary statistical ap...</td>\n",
       "      <td>Medicine and science in sports and exercise</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Li C</td>\n",
       "      <td>Predict the neural network mathematical model ...</td>\n",
       "      <td>Deep learning has achieved impressive predicti...</td>\n",
       "      <td>Physical review letters</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lu G</td>\n",
       "      <td>Evaluation model of young basketball players ’...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wu L</td>\n",
       "      <td>The participating team ’s technical analysis o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zhang Q</td>\n",
       "      <td>Prediction based on basketball competition vid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Author  \\\n",
       "0  López-Valenciano A, Ayala F, Puerta JM, et al   \n",
       "1                                           Li C   \n",
       "2                                           Lu G   \n",
       "3                                           Wu L   \n",
       "4                                        Zhang Q   \n",
       "\n",
       "                                               Title  \\\n",
       "0  A preventive model for muscle injuries: a nove...   \n",
       "1  Predict the neural network mathematical model ...   \n",
       "2  Evaluation model of young basketball players ’...   \n",
       "3  The participating team ’s technical analysis o...   \n",
       "4  Prediction based on basketball competition vid...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  The application of contemporary statistical ap...   \n",
       "1  Deep learning has achieved impressive predicti...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                       Journal    Year  \n",
       "0  Medicine and science in sports and exercise  2018.0  \n",
       "1                      Physical review letters  2020.0  \n",
       "2                                          NaN     NaN  \n",
       "3                                          NaN     NaN  \n",
       "4                                          NaN     NaN  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the CSV file with references\n",
    "ref_abs = pd.read_csv(os.path.join('..','results','refs_abstracts_sys.csv'))\n",
    "ref_abs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 2.1 Finding the Most Common Words in Abstract (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('team', 28), ('data', 27), ('performance', 26), ('classification', 19), ('ball', 18), ('match', 16), ('indicators', 15), ('football', 15), ('learning', 14), ('based', 13)]\n"
     ]
    }
   ],
   "source": [
    "#Function for cleaning and preprocessing the abstracts still using NLTK\n",
    "def preprocess_text(texts):\n",
    "    prepositions = set([\n",
    "    \n",
    "    ])\n",
    "    #Filtering out common words that are \"meaningless\" (such as prepositions) using stop words\n",
    "    stop_words = set(stopwords.words('english')).union(prepositions)\n",
    "\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            #Replace hyphens and slashes with spaces, then split the text into words\n",
    "            #Lowercasing, punctuation removal, word tokenization, and stop word filtering\n",
    "            words = text.lower().translate(str.maketrans('-/', '  ')).split()\n",
    "            #Split on other punctuations and filter out stop words\n",
    "            words = [word for part in words for word in part.translate(str.maketrans('', '', string.punctuation)).split() if word not in stop_words]\n",
    "            cleaned_texts.extend(words)\n",
    "\n",
    "    return cleaned_texts\n",
    "#Preprocessing abstracts and counting\n",
    "cleaned_abstracts = preprocess_text(ref_abs['Abstract'])\n",
    "word_counts = Counter(cleaned_abstracts)\n",
    "most_common_words_basic_A = word_counts.most_common(10)\n",
    "\n",
    "print(most_common_words_basic_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Method is limited, as words like \"sport\" and \"sports\" will be counted separately. We will further apply lemmatization and stemming to address this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Finding the Most Common Words in Abstract (Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('team', 35), ('perform', 33), ('use', 32), ('classifi', 28), ('data', 27), ('indic', 22), ('model', 21), ('classif', 19), ('match', 19), ('ball', 18)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "#Function for cleaning and preprocessing the abstracts\n",
    "def preprocess_text(texts):\n",
    "    prepositions = set([\n",
    "        \n",
    "    ])\n",
    "    #Filtering out common words that are \"meaningless\" (such as prepositions) using stop words\n",
    "    stop_words = set(stopwords.words('english')).union(prepositions)\n",
    "    stemmer = PorterStemmer()\n",
    "    #Using PorterStemmer() for stemming and further lowercasing, punctuation removal, word tokenization, and stop word filtering\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            #Replace hyphens and slashes with spaces\n",
    "            words = text.lower().translate(str.maketrans('-/', '  ')).split()\n",
    "            words = [word for part in words for word in part.translate(str.maketrans('', '', string.punctuation)).split() if word not in stop_words]\n",
    "            stemmed_words = [stemmer.stem(word) for word in words]\n",
    "            cleaned_texts.extend(stemmed_words)\n",
    "\n",
    "    return cleaned_texts\n",
    "#Preprocessing abstracts and counting\n",
    "cleaned_abstracts = preprocess_text(ref_abs['Abstract'])\n",
    "word_counts = Counter(cleaned_abstracts)\n",
    "most_common_words_stemming_A = word_counts.most_common(10)\n",
    "\n",
    "print(most_common_words_stemming_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Most Common words in Abstract (Lematization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/matteo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/matteo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('team', 35), ('data', 27), ('use', 27), ('performance', 26), ('model', 21), ('classification', 19), ('match', 19), ('ball', 18), ('indicator', 18), ('classify', 16)]\n"
     ]
    }
   ],
   "source": [
    "#Ensure necessary NLTK resources are downloaded\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#Function to map NLTK's part of speech tags to those used by WordNet\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to the first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "#Function for cleaning and preprocessing the abstracts with lemmatization\n",
    "def preprocess_text(texts):\n",
    "    prepositions = set([\n",
    "       \n",
    "    ])\n",
    "    stop_words = set(stopwords.words('english')).union(prepositions)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "#Lowercasing, punctuation removal, word tokenization, lemmatizing and stop word filtering\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            #Replace hyphens and slashes with spaces\n",
    "            words = text.lower().translate(str.maketrans('-/', '  ')).split()\n",
    "            words = [word for part in words for word in part.translate(str.maketrans('', '', string.punctuation)).split() if word not in stop_words]\n",
    "            lem_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "            cleaned_texts.extend(lem_words)\n",
    "\n",
    "    return cleaned_texts\n",
    "#Preprocessing the abstracts and counting\n",
    "cleaned_abstracts = preprocess_text(ref_abs['Abstract'])\n",
    "word_counts = Counter(cleaned_abstracts)\n",
    "most_common_words_lemm_A = word_counts.most_common(10)\n",
    "\n",
    "print(most_common_words_lemm_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Displaying All Three Methods Together and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ab1d0 th {\n",
       "  background-color: #f4f4f4;\n",
       "  color: black;\n",
       "}\n",
       "#T_ab1d0_row0_col0, #T_ab1d0_row0_col1, #T_ab1d0_row0_col2, #T_ab1d0_row0_col3, #T_ab1d0_row0_col4, #T_ab1d0_row0_col5, #T_ab1d0_row0_col6, #T_ab1d0_row0_col7, #T_ab1d0_row0_col8, #T_ab1d0_row0_col9, #T_ab1d0_row1_col0, #T_ab1d0_row1_col1, #T_ab1d0_row1_col2, #T_ab1d0_row1_col3, #T_ab1d0_row1_col4, #T_ab1d0_row1_col5, #T_ab1d0_row1_col6, #T_ab1d0_row1_col7, #T_ab1d0_row1_col8, #T_ab1d0_row1_col9, #T_ab1d0_row2_col0, #T_ab1d0_row2_col1, #T_ab1d0_row2_col2, #T_ab1d0_row2_col3, #T_ab1d0_row2_col4, #T_ab1d0_row2_col5, #T_ab1d0_row2_col6, #T_ab1d0_row2_col7, #T_ab1d0_row2_col8, #T_ab1d0_row2_col9 {\n",
       "  background-color: white;\n",
       "  color: black;\n",
       "  border-color: black;\n",
       "  border-style: solid;\n",
       "  border-width: 1px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ab1d0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ab1d0_level0_col0\" class=\"col_heading level0 col0\" >1st Most Common</th>\n",
       "      <th id=\"T_ab1d0_level0_col1\" class=\"col_heading level0 col1\" >2nd Most Common</th>\n",
       "      <th id=\"T_ab1d0_level0_col2\" class=\"col_heading level0 col2\" >3rd Most Common</th>\n",
       "      <th id=\"T_ab1d0_level0_col3\" class=\"col_heading level0 col3\" >4th Most Common</th>\n",
       "      <th id=\"T_ab1d0_level0_col4\" class=\"col_heading level0 col4\" >5th Most Common</th>\n",
       "      <th id=\"T_ab1d0_level0_col5\" class=\"col_heading level0 col5\" >6th Most Common</th>\n",
       "      <th id=\"T_ab1d0_level0_col6\" class=\"col_heading level0 col6\" >7th Most Common</th>\n",
       "      <th id=\"T_ab1d0_level0_col7\" class=\"col_heading level0 col7\" >8th Most Common</th>\n",
       "      <th id=\"T_ab1d0_level0_col8\" class=\"col_heading level0 col8\" >9th Most Common</th>\n",
       "      <th id=\"T_ab1d0_level0_col9\" class=\"col_heading level0 col9\" >10th Most Common</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Method</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ab1d0_level0_row0\" class=\"row_heading level0 row0\" >Basic</th>\n",
       "      <td id=\"T_ab1d0_row0_col0\" class=\"data row0 col0\" >team</td>\n",
       "      <td id=\"T_ab1d0_row0_col1\" class=\"data row0 col1\" >data</td>\n",
       "      <td id=\"T_ab1d0_row0_col2\" class=\"data row0 col2\" >performance</td>\n",
       "      <td id=\"T_ab1d0_row0_col3\" class=\"data row0 col3\" >classification</td>\n",
       "      <td id=\"T_ab1d0_row0_col4\" class=\"data row0 col4\" >ball</td>\n",
       "      <td id=\"T_ab1d0_row0_col5\" class=\"data row0 col5\" >match</td>\n",
       "      <td id=\"T_ab1d0_row0_col6\" class=\"data row0 col6\" >indicators</td>\n",
       "      <td id=\"T_ab1d0_row0_col7\" class=\"data row0 col7\" >football</td>\n",
       "      <td id=\"T_ab1d0_row0_col8\" class=\"data row0 col8\" >learning</td>\n",
       "      <td id=\"T_ab1d0_row0_col9\" class=\"data row0 col9\" >based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab1d0_level0_row1\" class=\"row_heading level0 row1\" >Stemming</th>\n",
       "      <td id=\"T_ab1d0_row1_col0\" class=\"data row1 col0\" >team</td>\n",
       "      <td id=\"T_ab1d0_row1_col1\" class=\"data row1 col1\" >perform</td>\n",
       "      <td id=\"T_ab1d0_row1_col2\" class=\"data row1 col2\" >use</td>\n",
       "      <td id=\"T_ab1d0_row1_col3\" class=\"data row1 col3\" >classifi</td>\n",
       "      <td id=\"T_ab1d0_row1_col4\" class=\"data row1 col4\" >data</td>\n",
       "      <td id=\"T_ab1d0_row1_col5\" class=\"data row1 col5\" >indic</td>\n",
       "      <td id=\"T_ab1d0_row1_col6\" class=\"data row1 col6\" >model</td>\n",
       "      <td id=\"T_ab1d0_row1_col7\" class=\"data row1 col7\" >classif</td>\n",
       "      <td id=\"T_ab1d0_row1_col8\" class=\"data row1 col8\" >match</td>\n",
       "      <td id=\"T_ab1d0_row1_col9\" class=\"data row1 col9\" >ball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab1d0_level0_row2\" class=\"row_heading level0 row2\" >Lemmatization</th>\n",
       "      <td id=\"T_ab1d0_row2_col0\" class=\"data row2 col0\" >team</td>\n",
       "      <td id=\"T_ab1d0_row2_col1\" class=\"data row2 col1\" >data</td>\n",
       "      <td id=\"T_ab1d0_row2_col2\" class=\"data row2 col2\" >use</td>\n",
       "      <td id=\"T_ab1d0_row2_col3\" class=\"data row2 col3\" >performance</td>\n",
       "      <td id=\"T_ab1d0_row2_col4\" class=\"data row2 col4\" >model</td>\n",
       "      <td id=\"T_ab1d0_row2_col5\" class=\"data row2 col5\" >classification</td>\n",
       "      <td id=\"T_ab1d0_row2_col6\" class=\"data row2 col6\" >match</td>\n",
       "      <td id=\"T_ab1d0_row2_col7\" class=\"data row2 col7\" >ball</td>\n",
       "      <td id=\"T_ab1d0_row2_col8\" class=\"data row2 col8\" >indicator</td>\n",
       "      <td id=\"T_ab1d0_row2_col9\" class=\"data row2 col9\" >classify</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f64b3f8c5d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using pandas for displaying the results side by side in a table for ease of comparison\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame({\n",
    "    'Method': ['Basic', 'Stemming', 'Lemmatization'],\n",
    "    '1st Most Common': [most_common_words_basic_A[0][0], most_common_words_stemming_A[0][0], most_common_words_lemm_A[0][0]],\n",
    "    '2nd Most Common': [most_common_words_basic_A[1][0], most_common_words_stemming_A[1][0], most_common_words_lemm_A[1][0]],\n",
    "    '3rd Most Common': [most_common_words_basic_A[2][0], most_common_words_stemming_A[2][0], most_common_words_lemm_A[2][0]],\n",
    "    '4th Most Common': [most_common_words_basic_A[3][0], most_common_words_stemming_A[3][0], most_common_words_lemm_A[3][0]],\n",
    "    '5th Most Common': [most_common_words_basic_A[4][0], most_common_words_stemming_A[4][0], most_common_words_lemm_A[4][0]],\n",
    "    '6th Most Common': [most_common_words_basic_A[5][0], most_common_words_stemming_A[5][0], most_common_words_lemm_A[5][0]],\n",
    "    '7th Most Common': [most_common_words_basic_A[6][0], most_common_words_stemming_A[6][0], most_common_words_lemm_A[6][0]],\n",
    "    '8th Most Common': [most_common_words_basic_A[7][0], most_common_words_stemming_A[7][0], most_common_words_lemm_A[7][0]],\n",
    "    '9th Most Common': [most_common_words_basic_A[8][0], most_common_words_stemming_A[8][0], most_common_words_lemm_A[8][0]],\n",
    "    '10th Most Common': [most_common_words_basic_A[9][0], most_common_words_stemming_A[9][0], most_common_words_lemm_A[9][0]]\n",
    "})\n",
    "results_df.set_index('Method', inplace=True)\n",
    "\n",
    "#Applying basic styling\n",
    "styled_df = results_df.style.set_properties(**{\n",
    "    'background-color': 'white',  #Background color\n",
    "    'color': 'black',             #Font color\n",
    "    'border-color': 'black',      #Border color\n",
    "    'border-style': 'solid',      #Border style\n",
    "    'border-width': '1px'         #Border width\n",
    "}).set_table_styles([{\n",
    "    'selector': 'th',\n",
    "    'props': [('background-color', '#f4f4f4'), ('color', 'black')]  #Header styling\n",
    "}])\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conclusion:***  Lemmatization considers the context and part of speech of a word, leading to more accurate results as it reduces words to their dictionary form, but requires more computational power. Stemming is robust for search and indexing purposes as the exact form of a word is less important and is faster. It's interesting to note that the word \"data is consistently the most common word across all 3 methods in the abstract and some words like \"risk\" have greatly varying positioning. Overall, compared to titles the differences are less extreme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accross both title and abstract, the most common words vary significantly. In the 3rd part we will discuss what are the most common words in both together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Title and Abstract Together"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
